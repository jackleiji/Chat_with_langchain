# å¯¼å…¥å¿…è¦çš„åº“

import sys
import os                # ç”¨äºæ“ä½œç³»ç»Ÿç›¸å…³çš„æ“ä½œï¼Œä¾‹å¦‚è¯»å–ç¯å¢ƒå˜é‡

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import IPython.display   # ç”¨äºåœ¨ IPython ç¯å¢ƒä¸­æ˜¾ç¤ºæ•°æ®ï¼Œä¾‹å¦‚å›¾ç‰‡
import io                # ç”¨äºå¤„ç†æµå¼æ•°æ®ï¼ˆä¾‹å¦‚æ–‡ä»¶æµï¼‰
import gradio as gr
from dotenv import load_dotenv, find_dotenv
from llm.call_llm import get_completion
from database.create_db import create_db_info
from qa_chain.Chat_QA_chain_self import Chat_QA_chain_self
from qa_chain.QA_chain_self import QA_chain_self
import re
import warnings
 
warnings.filterwarnings("ignore")
# å¯¼å…¥ dotenv åº“çš„å‡½æ•°
# dotenv å…è®¸æ‚¨ä» .env æ–‡ä»¶ä¸­è¯»å–ç¯å¢ƒå˜é‡
# è¿™åœ¨å¼€å‘æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå¯ä»¥é¿å…å°†æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚APIå¯†é’¥ï¼‰ç¡¬ç¼–ç åˆ°ä»£ç ä¸­

# å¯»æ‰¾ .env æ–‡ä»¶å¹¶åŠ è½½å®ƒçš„å†…å®¹
# è¿™å…è®¸æ‚¨ä½¿ç”¨ os.environ æ¥è¯»å–åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®çš„ç¯å¢ƒå˜é‡
_ = load_dotenv(find_dotenv())
LLM_MODEL_DICT = {
    "openai": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-0613", "gpt-4", "gpt-4-32k"],
    "wenxin": ["ERNIE-Bot", "ERNIE-Bot-4", "ERNIE-Bot-turbo"],
    "xinhuo": ["Spark-1.5", "Spark-2.0"],
    # "zhipuai": ["chatglm_pro", "chatglm_std", "chatglm_lite"]
    "zhipuai": ["glm-4-plus", "glm-4-air", "chatglm_lite"]
}


LLM_MODEL_LIST = sum(list(LLM_MODEL_DICT.values()),[])
INIT_LLM = "glm-4-plus"
EMBEDDING_MODEL_LIST = ['zhipuai', 'm3e']
INIT_EMBEDDING_MODEL = "m3e"
DEFAULT_DB_PATH = "./knowledge_db"
DEFAULT_PERSIST_PATH = "./vector_db"
AIGC_AVATAR_PATH = "./figures/aigc_avatar.png"
DATAWHALE_AVATAR_PATH = "./figures/datawhale_avatar.png"
AIGC_LOGO_PATH = "./figures/aigc_logo.png"
DATAWHALE_LOGO_PATH = "./figures/datawhale_logo.png"
HEAD_IMAGE_PATH = "./figures/head.png"

def get_model_by_platform(platform):
    return LLM_MODEL_DICT.get(platform, "")

class Model_center():
    """
    å­˜å‚¨é—®ç­” Chain çš„å¯¹è±¡ 

    - chat_qa_chain_self: ä»¥ (model, embedding) ä¸ºé”®å­˜å‚¨çš„å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ã€‚
    - qa_chain_self: ä»¥ (model, embedding) ä¸ºé”®å­˜å‚¨çš„ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ã€‚
    """
    def __init__(self):
        self.chat_qa_chain_self = {}
        self.qa_chain_self = {}

    def chat_qa_chain_self_answer(self, question, chat_history: list = [], model="glm-4-plus", embedding="zhipuai", temperature=0.0, top_k=4, history_len=3, file_path=DEFAULT_DB_PATH, persist_path=DEFAULT_PERSIST_PATH):
        """å¸¦å†å²è®°å½•çš„é—®ç­”"""
        if not question or len(str(question).strip()) == 0:
            return "", chat_history
        
        try:
            progress = gr.Progress()
            progress(0, desc="æ­£åœ¨å‡†å¤‡æ¨¡å‹...")
            
            if (model, embedding) not in self.chat_qa_chain_self:
                progress(0.2, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
                self.chat_qa_chain_self[(model, embedding)] = Chat_QA_chain_self(
                    model=model,
                    temperature=temperature,
                    top_k=top_k,
                    chat_history=chat_history,
                    file_path=file_path,
                    persist_path=persist_path,
                    embedding=embedding
                )
            
            chain = self.chat_qa_chain_self[(model, embedding)]
            progress(0.5, desc="æ­£åœ¨ç”Ÿæˆå›ç­”...")
            answer = chain.answer(question=str(question), temperature=temperature, top_k=top_k)
            
            if not isinstance(answer, str):
                answer = str(answer)
            
            chat_history.append((str(question), answer))
            progress(1.0, desc="å®Œæˆ!")
            return "", chat_history
            
        except Exception as e:
            error_message = f"Error: {str(e)}"
            if chat_history is None:
                chat_history = []
            chat_history.append((str(question), error_message))
            return "", chat_history

    def qa_chain_self_answer(self, question, chat_history: list = [], model="openai", embedding="openai", temperature=0.0, top_k=4, file_path=DEFAULT_DB_PATH, persist_path=DEFAULT_PERSIST_PATH):
        """ä¸å¸¦å†å²è®°å½•çš„é—®ç­”"""
        if not question or len(str(question).strip()) == 0:
            return "", chat_history
            
        try: 
            progress = gr.Progress()
            progress(0, desc="æ­£åœ¨å‡†å¤‡æ¨¡å‹...")
            
            # è·å– API key
            # model_key = parse_llm_api_key(model)  # ä½¿ç”¨å¯¼å…¥çš„å‡½æ•°
            
            if (model, embedding) not in self.qa_chain_self:
                progress(0.2, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
                self.qa_chain_self[(model, embedding)] = QA_chain_self(
                    model=model,
                    temperature=temperature,
                    top_k=top_k,
                    file_path=file_path,
                    persist_path=persist_path,
                    embedding=embedding
                )
            
            chain = self.qa_chain_self[(model, embedding)]
            progress(0.5, desc="æ­£åœ¨ç”Ÿæˆå›ç­”...")
            answer = chain.answer(question=str(question), temperature=temperature, top_k=top_k)
            
            if not isinstance(answer, str):
                answer = str(answer)
            
            chat_history.append((str(question), answer))
            progress(1.0, desc="å®Œæˆ!")
            return "", chat_history
            
        except Exception as e:
            error_message = f"Error: {str(e)}"
            if chat_history is None:
                chat_history = []
            chat_history.append((str(question), error_message))
            return "", chat_history

    def clear_history(self):
        if len(self.chat_qa_chain_self) > 0:
            for chain in self.chat_qa_chain_self.values():
                chain.clear_history()


def format_chat_prompt(message, chat_history):
    """
    è¯¥å‡½æ•°ç”¨äºæ ¼å¼åŒ–èŠå¤© promptã€‚

    å‚æ•°:
    message: å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
    chat_history: èŠå¤©å†å²è®°å½•ã€‚

    è¿”å›:
    prompt: æ ¼å¼åŒ–åçš„ promptã€‚
    """
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œç”¨äºå­˜æ”¾æ ¼å¼åŒ–åçš„èŠå¤© promptã€‚
    prompt = "ä½ æ˜¯PHMå¹³å°çš„å°è‰¾åŠ©æ‰‹"
    # éå†èŠå¤©å†å²è®°å½•ã€‚
    for turn in chat_history:
        # ä»èŠå¤©è®°å½•ä¸­æå–ç”¨æˆ·å’Œæœºå™¨äººçš„æ¶ˆæ¯ã€‚
        user_message, bot_message = turn
        # æ›´æ–° promptï¼ŒåŠ å…¥ç”¨æˆ·å’Œæœºå™¨äººçš„æ¶ˆæ¯ã€‚
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    # å°†å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ä¹ŸåŠ å…¥åˆ° promptä¸­ï¼Œå¹¶é¢„ç•™ä¸€ä¸ªä½ç½®ç»™æœºå™¨äººçš„å›å¤ã€‚
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    # è¿”å›æ ¼å¼åçš„ promptã€‚
    return prompt



def respond(message, chat_history, llm, history_len=3, temperature=0.1):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›å“åº”"""
    if not message or len(message.strip()) == 0:
        return "", chat_history
    try:
        # ç¡®ä¿ chat_history æ˜¯åˆ—è¡¨
        if chat_history is None:
            chat_history = []
            
        # æ ¼å¼åŒ–èŠå¤©è®°å½•
        prompt = format_chat_prompt(message, chat_history[-history_len:] if history_len > 0 else [])
        
        # è·å– AI å›å¤
        bot_message = get_completion(
            prompt=prompt,
            model=llm,
            temperature=temperature
        )
        
        # ç¡®ä¿è¿”å›çš„æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(bot_message, str):
            bot_message = str(bot_message)
            
        # æ·»åŠ åˆ°èŠå¤©å†å²
        chat_history.append((str(message), bot_message))
        
        return "", chat_history
    except Exception as e:
        error_message = f"Error: {str(e)}"
        chat_history.append((str(message), error_message))
        return "", chat_history


model_center = Model_center()

# ä¿®æ”¹ CSS æ ·å¼
CSS = """
/* ç°è‰²èƒŒæ™¯ */
.gradio-container {
    background-color: #f5f5f5 !important;
    max-width: 1500px !important;  /* è®¾ç½®æœ€å¤§å®½åº¦ */
    margin: 0 auto !important;     /* æ°´å¹³å±…ä¸­ */
    padding: 20px !important;      /* æ·»åŠ å†…è¾¹è· */
}

/* å†…å®¹åŒºåŸŸæ ·å¼ */
.contain {
    background-color: white;
    border-radius: 8px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    padding: 16px;
    margin: 16px 0;
    width: 100%;  /* ç¡®ä¿å†…å®¹åŒºåŸŸå¡«å……å®¹å™¨å®½åº¦ */
}

/* èŠå¤©çª—å£æ ·å¼ */
.custom-chatbot {
    font-size: 10.5pt !important;
    line-height: 1.5 !important;
    width: 100% !important;        /* ç¡®ä¿èŠå¤©çª—å£å¡«å……å®¹å™¨å®½åº¦ */
    max-width: 1200px !important;   /* èŠå¤©çª—å£æœ€å¤§å®½åº¦ */
    margin: 0 auto !important;     /* èŠå¤©çª—å£å±…ä¸­ */
}

/* è°ƒæ•´åˆ—å¸ƒå±€ */
.main-cols {
    gap: 20px !important;          /* åˆ—ä¹‹é—´çš„é—´è· */
    align-items: start !important; /* åˆ—é¡¶éƒ¨å¯¹é½ */
}

/* å³ä¾§æ§åˆ¶é¢æ¿æ ·å¼ */
.control-panel {
    background-color: white;
    border-radius: 8px;
    padding: 16px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    width: 100%;                   /* å¡«å……åˆ—å®½åº¦ */
}

/* é¡µè„šæ ·å¼ */
.footer-container {
    display: flex;
    justify-content: space-between;
    padding: 20px;
    background-color: white;
    border-radius: 8px;
    margin-top: 20px;
    width: 100%;                   /* ç¡®ä¿é¡µè„šå¡«å……å®¹å™¨å®½åº¦ */
    max-width: 1160px !important;  /* é¡µè„šæœ€å¤§å®½åº¦ */
    margin-left: auto !important;
    margin-right: auto !important;
}

/* ç‚«é…·çš„æ ‡é¢˜æ ·å¼ */
.artistic-title {
    font-family: 'Arial', sans-serif;
    font-size: 2.5em;
    font-weight: bold;
    background: linear-gradient(120deg, 
        #1a237e 0%, 
        #0d47a1 25%,
        #0288d1 50%,
        #0097a7 75%,
        #00838f 100%
    );
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    text-shadow: 3px 3px 6px rgba(0,0,0,0.2);
    position: relative;
    padding: 10px 0;
}

.artistic-title::after {
    content: '';
    position: absolute;
    bottom: 0;
    left: 25%;
    width: 50%;
    height: 2px;
    background: linear-gradient(90deg, 
        transparent 0%,
        #2196F3 50%,
        transparent 100%
    );
}

.subtitle {
    font-family: 'Arial', sans-serif;
    color: #455a64;
    font-size: 1.2em;
    letter-spacing: 5px;
    text-transform: uppercase;
    margin-top: 10px;
    background: linear-gradient(45deg, #455a64, #78909c);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}
"""

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(css=CSS) as demo:
    # Add header image
    gr.Image(value=HEAD_IMAGE_PATH, scale=1, show_label=False, show_download_button=False)
    
    # Existing logo row
    with gr.Row(equal_height=True, elem_classes="main-cols"):           
        gr.Image(value=AIGC_LOGO_PATH, scale=1, min_width=10, show_label=False, show_download_button=False, container=False)
   
        with gr.Column(scale=2):
            gr.Markdown("""
                <div style="text-align: center; padding: 20px 0;">
                    <h1 class="artistic-title">åŸºäºLLMçš„æ™ºèƒ½ä¸“åˆ©æ£€ç´¢åŠ©ç†</h1>
                    <div class="subtitle">LLM-UNIVERSE</div>
                </div>
                """)
        gr.Image(value=DATAWHALE_LOGO_PATH, scale=1, min_width=10, show_label=False, show_download_button=False, container=False)

    with gr.Row():
        with gr.Column(scale=4):
            # ä¿®æ”¹èŠå¤©çª—å£é…ç½®
            chatbot = gr.Chatbot(
                height=600,  # è°ƒæ•´é«˜åº¦ä¸º500px
                show_copy_button=True, 
                show_share_button=True, 
                avatar_images=(AIGC_AVATAR_PATH, DATAWHALE_AVATAR_PATH),
                elem_classes="custom-chatbot"  # åº”ç”¨è‡ªå®šä¹‰æ ·å¼
            )
            
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
            msg = gr.Textbox(label="Prompt/é—®é¢˜")

            with gr.Row():
                button_state = gr.State("")  # æ·»åŠ çŠ¶æ€ä»¶æ¥è·Ÿè¸ªå½“å‰é€‰ä¸­çš„æŒ‰é’®
                db_with_his_btn = gr.Button(
                    "Chat db with history",
                    elem_classes=["custom-button"],
                    variant="primary"
                )
                db_wo_his_btn = gr.Button(
                    "Chat db without history",
                    elem_classes=["custom-button"],
                    variant="primary"
                )
                llm_btn = gr.Button(
                    "Chat with llm",
                    elem_classes=["custom-button"],
                    variant="primary"
                )
            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                clear = gr.ClearButton(
                    components=[chatbot], value="Clear console")

        with gr.Column(scale=1, elem_classes="control-panel"):
            # ä¿®æ”¹æ–‡ä»¶é€‰æ‹©å™¨é…ç½®
            file = gr.File(
                label='è¯·é€‰æ‹©çŸ¥è¯†åº“æ–‡ä»¶',
                file_count='multiple',
                file_types=['.txt', '.md', '.docx', '.doc', '.pdf'],
                show_label=True,
                container=True,
                scale=1,
                min_width=None,
                interactive=True,
                visible=True,
                elem_id="file_upload"
            )
            
            with gr.Row():
                init_db = gr.Button("çŸ¥è¯†åº“æ–‡ä»¶å‘é‡åŒ–", variant="primary")
                init_status = gr.Textbox(
                    label="å‘é‡åŒ–çŠ¶æ€",
                    placeholder="è¯·ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹å‘é‡åŒ–...",
                    interactive=False,
                    show_label=True
                )
            
            model_argument = gr.Accordion("å‚æ•°é…ç½®", open=False)
            with model_argument:
                temperature = gr.Slider(0,
                                        1,
                                        value=0.01,
                                        step=0.01,
                                        label="llm temperature",
                                        interactive=True)

                top_k = gr.Slider(1,
                                  10,
                                  value=3,
                                  step=1,
                                  label="vector db search top k",
                                  interactive=True)

                history_len = gr.Slider(0,
                                        5,
                                        value=3,
                                        step=1,
                                        label="history length",
                                        interactive=True)

            model_select = gr.Accordion("æ¨¡å‹é€‰æ‹©")
            with model_select:
                llm = gr.Dropdown(
                    LLM_MODEL_LIST,
                    label="large language model",
                    value=INIT_LLM,
                    interactive=True)

                embeddings = gr.Dropdown(EMBEDDING_MODEL_LIST,
                                         label="Embedding model",
                                         value=INIT_EMBEDDING_MODEL)

        # ä¿®æ”¹æŒ‰é’®ç‚¹å‡»äº‹ä»¶çš„å¤„ç†æ–¹å¼
        # æŒ‰é’®çŠ¶æ€åˆ‡æ¢äº‹ä»¶
        db_with_his_btn.click(
            fn=model_center.chat_qa_chain_self_answer,
            inputs=[msg, chatbot, llm, embeddings, temperature, top_k, history_len],
            outputs=[msg, chatbot],
            show_progress="full"
        )

        db_wo_his_btn.click(
            fn=model_center.qa_chain_self_answer,
            inputs=[msg, chatbot, llm, embeddings, temperature, top_k],
            outputs=[msg, chatbot],
            show_progress="full"
        )

        llm_btn.click(
            fn=respond,
            inputs=[msg, chatbot, llm, history_len, temperature],
            outputs=[msg, chatbot],
            show_progress="full"
        )

        # åˆå§‹åŒ–æ•°æ®åº“æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        init_db.click(
            fn=create_db_info,
            inputs=[file, embeddings],
            outputs=init_status
        )

        msg.submit(
            respond,
            inputs=[msg, chatbot, llm, history_len, temperature],
            outputs=[msg, chatbot],
            show_progress="full"
        )

        clear.click(model_center.clear_history)

    # æ·»åŠ é¡µè„šæ¯
    gr.Markdown(
        """
        <div class="footer-container">
            <div class="footer-left">
                <div class="footer-content">æé†’ï¼š</div>
                <div class="footer-content">1. ä½¿ç”¨æ—¶è¯·å…ˆä¸Šä¼ è‡ªå·±çš„çŸ¥è¯†æ–‡ä»¶ï¼Œä¸ç„¶å°†ä¼šè§£æé¡¹ç›®è‡ªå¸¦çš„çŸ¥è¯†åº“ã€‚</div>
                <div class="footer-content">2. åˆå§‹åŒ–æ•°æ®åº“æ—¶é—´å¯èƒ½è¾ƒé•¿ï¼Œè¯·è€ç­‰å¾…ã€‚</div>
                <div class="footer-content">3. ä½¿ç”¨ä¸­å¦‚æœå‡ºç°å¼‚å¸¸ï¼Œå°†ä¼šåœ¨æ–‡æœ¬å…¥æ¡†è¿›è¡Œå±•ç¤ºï¼Œè¯·ä¸è¦æƒŠæ…Œã€‚</div>
            </div>
            <div class="footer-right">
                <div class="footer-content">ğŸ¤– LLMæ™ºèƒ½åŠ©æ‰‹ - æ‚¨çš„æ™ºèƒ½æ–‡çŒ®æ£€ç´¢ä¸“å®¶</div>
                <div class="footer-content">ğŸ“š å¿«é€Ÿã€å‡†ç¡®ã€é«˜æ•ˆ - æå‡æ‚¨çš„æ–‡çŒ®æ£€ç´¢æ•ˆç‡</div>
                <div class="footer-content">ğŸ’¡ æ”¯æŒå¤šç§æ–‡çŒ®æ ¼å¼ï¼Œæ™ºèƒ½åˆ†æï¼Œç²¾å‡†ç­”å¤</div>
            </div>
        </div>
        """)

# æ·»åŠ JavaScriptä»£ç æ¥å¤„ç†æŒ‰é’®çŠ¶æ€
js_code = """
function toggleButton(btnId) {
    const buttons = document.querySelectorAll('.custom-button');
    let selectedText = '';
    
    buttons.forEach(btn => {
        if (btn.id === btnId) {
            const wasSelected = btn.classList.contains('selected');
            btn.classList.toggle('selected');
            if (!wasSelected) {
                selectedText = 'å·²é€‰æ‹©: ' + btn.textContent;
            }
        } else {
            btn.classList.remove('selected');
        }
    });
    
    // æ˜¾ç¤ºæç¤ºä¿¡æ¯
    const toast = document.createElement('div');
    toast.className = 'toast-message';
    toast.textContent = selectedText;
    document.body.appendChild(toast);
    
    // 2ç§’åç§»é™¤æç¤º
    setTimeout(() => {
        toast.classList.add('toast-fade-out');
        setTimeout(() => {
            document.body.removeChild(toast);
        }, 300);
    }, 2000);
    
    return true;
}
"""

# å¯åŠ¨åº”ç”¨
demo.launch(
    share=False,
    server_port=7860,
    server_name="127.0.0.1",
    favicon_path="./figures/aigc_logo.png"  # ä¿ç•™æ”¯æŒçš„å‚æ•°
)
